{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train a Medical Specialty Detector on SageMaker Using HuggingFace Transformers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this workshop, we will show how you can train an NLP classifier using trainsformers from [HuggingFace](https://huggingface.co/). HuggingFace allows for easily using prebuilt transformers, which you can train for your own use cases. \n",
    "\n",
    "In this workshop, we will use the SageMaker HuggingFace supplied container to train an algorithm that will distinguish between physician notes that are either part of the General Medicine (encoded as 0), or Radiology (encoded as 1) medical specialties. The data is a subsample from [MTSamples](https://www.mtsamples.com/) which was downloaded from [here](https://www.kaggle.com/tboyle10/medicaltranscriptions)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pip.repos.neuron.amazonaws.com\n",
      "Collecting transformers\n",
      "  Using cached transformers-4.19.2-py3-none-any.whl (4.2 MB)\n",
      "Requirement already satisfied: seaborn in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (0.11.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from transformers) (1.20.3)\n",
      "Requirement already satisfied: requests in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from transformers) (2.26.0)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from transformers) (4.62.3)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from transformers) (21.0)\n",
      "Collecting huggingface-hub<1.0,>=0.1.0\n",
      "  Using cached huggingface_hub-0.7.0-py3-none-any.whl (86 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from transformers) (2021.11.10)\n",
      "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
      "  Using cached tokenizers-0.12.1-cp38-cp38-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: filelock in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from transformers) (3.4.0)\n",
      "Requirement already satisfied: matplotlib>=2.2 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from seaborn) (3.4.3)\n",
      "Requirement already satisfied: scipy>=1.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from seaborn) (1.7.2)\n",
      "Requirement already satisfied: pandas>=0.23 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from seaborn) (1.3.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (3.10.0.2)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (9.0.1)\n",
      "Requirement already satisfied: pyparsing>=2.2.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (3.0.6)\n",
      "Requirement already satisfied: cycler>=0.10 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
      "Requirement already satisfied: pytz>=2017.3 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from pandas>=0.23->seaborn) (2021.3)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests->transformers) (2.0.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests->transformers) (1.26.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests->transformers) (2021.10.8)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from requests->transformers) (3.1)\n",
      "Requirement already satisfied: six>=1.5 in /home/ec2-user/anaconda3/envs/tensorflow2_p38/lib/python3.8/site-packages (from python-dateutil>=2.7->matplotlib>=2.2->seaborn) (1.16.0)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.7.0 tokenizers-0.12.1 transformers-4.19.2\n",
      "\u001b[33mWARNING: You are using pip version 22.0.4; however, version 22.1.2 is available.\n",
      "You should consider upgrading via the '/home/ec2-user/anaconda3/envs/tensorflow2_p38/bin/python -m pip install --upgrade pip' command.\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install transformers seaborn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import sagemaker\n",
    "from sklearn.model_selection import train_test_split\n",
    "pd.set_option('max_colwidth', 500) # to allow for better display of dataframes\n",
    "sess = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "sagemaker_session = sagemaker.Session()\n",
    "BUCKET=sagemaker_session.default_bucket()\n",
    "PREFIX='mtsample_speciality_prediction'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read in data and examine sample data\n",
    "First we will read in the data; we will then copy it to S3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The data has 199 rows\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>specialty_encoded</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>CHIEF COMPLAINT:,  Weak and shaky.,HISTORY OF PRESENT ILLNESS:,  The patient is a 75-year-old, Caucasian female who comes in today with complaint of feeling weak and shaky.  When questioned further, she described shortness of breath primarily with ambulation.  She denies chest pain.  She denies cough, hemoptysis, dyspnea, and wheeze.  She denies syncope, presyncope, or palpitations.  Her symptoms are fairly longstanding but have been worsening as of late.,PAST MEDICAL HISTORY:,  She has had ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>1</td>\n",
       "      <td>INDICATION FOR STUDY: , Elevated cardiac enzymes, fullness in chest, abnormal EKG, and risk factors.,MEDICATIONS:,  Femara, verapamil, Dyazide, Hyzaar, glyburide, and metformin.,BASELINE EKG: , Sinus rhythm at 84 beats per minute, poor anteroseptal R-wave progression, mild lateral ST abnormalities.,EXERCISE RESULTS:,1.  The patient exercised for 3 minutes stopping due to fatigue.  No chest pain.,2.  Heart rate increased from 84 to 138 or 93% of maximum predicted heart rate.  Blood pressure r...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     specialty_encoded  \\\n",
       "8                    0   \n",
       "149                  1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                    text  \n",
       "8    CHIEF COMPLAINT:,  Weak and shaky.,HISTORY OF PRESENT ILLNESS:,  The patient is a 75-year-old, Caucasian female who comes in today with complaint of feeling weak and shaky.  When questioned further, she described shortness of breath primarily with ambulation.  She denies chest pain.  She denies cough, hemoptysis, dyspnea, and wheeze.  She denies syncope, presyncope, or palpitations.  Her symptoms are fairly longstanding but have been worsening as of late.,PAST MEDICAL HISTORY:,  She has had ...  \n",
       "149  INDICATION FOR STUDY: , Elevated cardiac enzymes, fullness in chest, abnormal EKG, and risk factors.,MEDICATIONS:,  Femara, verapamil, Dyazide, Hyzaar, glyburide, and metformin.,BASELINE EKG: , Sinus rhythm at 84 beats per minute, poor anteroseptal R-wave progression, mild lateral ST abnormalities.,EXERCISE RESULTS:,1.  The patient exercised for 3 minutes stopping due to fatigue.  No chest pain.,2.  Heart rate increased from 84 to 138 or 93% of maximum predicted heart rate.  Blood pressure r...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_1=pd.read_csv('MTsample_input_data.csv')\n",
    "print(f'''The data has {df_1.shape[0]} rows''')\n",
    "X_train, X_test = train_test_split(df_1, test_size=0.3)\n",
    "X_train.to_csv('train.csv')\n",
    "X_test.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./train.csv to s3://sagemaker-ap-southeast-2-431579215499/mtsample_speciality_prediction/train.csv\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp train.csv  s3://$BUCKET/$PREFIX/ #Copy the data to S3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configure the SageMaker training job\n",
    "We will leverage the SageMaker provided container definition to build and train the transformer. In this approach we specify our training script (`train.py`) but rely on the SageMaker HuggingFace container.\n",
    "\n",
    "For more information, see [here](https://docs.aws.amazon.com/sagemaker/latest/dg/hugging-face.html) and [here](https://huggingface.co/docs/sagemaker/main).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.huggingface import HuggingFace\n",
    "\n",
    "# hyperparameters which are passed to the training job\n",
    "hyperparameters={}\n",
    "\n",
    "# create the Estimator\n",
    "huggingface_estimator = HuggingFace(\n",
    "        entry_point='train.py',\n",
    "        instance_type='ml.g4dn.xlarge',\n",
    "        instance_count=1,\n",
    "        role=role,\n",
    "        transformers_version='4.11.0',\n",
    "        tensorflow_version='2.5.1',\n",
    "        py_version='py37',\n",
    "        hyperparameters = hyperparameters\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mpandas\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mpd\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtensorflow\u001b[39;49;00m \u001b[34mas\u001b[39;49;00m \u001b[04m\u001b[36mtf\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36margparse\u001b[39;49;00m\n",
      "\u001b[34mimport\u001b[39;49;00m \u001b[04m\u001b[36mos\u001b[39;49;00m\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m DistilBertTokenizer\n",
      "\u001b[34mfrom\u001b[39;49;00m \u001b[04m\u001b[36mtransformers\u001b[39;49;00m \u001b[34mimport\u001b[39;49;00m TFDistilBertForSequenceClassification\n",
      "\n",
      "\u001b[34mif\u001b[39;49;00m \u001b[31m__name__\u001b[39;49;00m == \u001b[33m\"\u001b[39;49;00m\u001b[33m__main__\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m:\n",
      "\n",
      "    parser = argparse.ArgumentParser()\n",
      "\n",
      "    \u001b[37m# hyperparameters sent by the client are passed as command-line arguments to the script.\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--epochs\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m3\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--train_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m32\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--eval_batch_size\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m64\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--warmup_steps\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mint\u001b[39;49;00m, default=\u001b[34m500\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_name\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m)\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--learning_rate\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34m5e-5\u001b[39;49;00m)\n",
      "\n",
      "    \u001b[37m# Data, model, and output directories\u001b[39;49;00m\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--output_data_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_OUTPUT_DATA_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--model_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_MODEL_DIR\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--n_gpus\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_NUM_GPUS\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--training_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=os.environ[\u001b[33m\"\u001b[39;49;00m\u001b[33mSM_CHANNEL_TRAIN\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    parser.add_argument(\u001b[33m\"\u001b[39;49;00m\u001b[33m--test_dir\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m, \u001b[36mtype\u001b[39;49;00m=\u001b[36mstr\u001b[39;49;00m, default=\u001b[34mNone\u001b[39;49;00m)\n",
      "\n",
      "    MODEL_NAME = \u001b[33m'\u001b[39;49;00m\u001b[33mdistilbert-base-uncased-finetuned-sst-2-english\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\n",
      "    BATCH_SIZE = \u001b[34m16\u001b[39;49;00m\n",
      "    N_EPOCHS = \u001b[34m3\u001b[39;49;00m\n",
      "\n",
      "    args, _ = parser.parse_known_args()\n",
      "\n",
      "\n",
      "    df_1=pd.read_csv(\u001b[33mf\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m\u001b[33m{\u001b[39;49;00margs.training_dir\u001b[33m}\u001b[39;49;00m\u001b[33m/train.csv\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m)\n",
      "\n",
      "    X_train=df_1\n",
      "    y_train=X_train[\u001b[33m'\u001b[39;49;00m\u001b[33mspecialty_encoded\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]\n",
      "    \u001b[37m#define a tokenizer object\u001b[39;49;00m\n",
      "    tokenizer = DistilBertTokenizer.from_pretrained(MODEL_NAME)\n",
      "    \u001b[37m#tokenize the text\u001b[39;49;00m\n",
      "    train_encodings = tokenizer(\u001b[36mlist\u001b[39;49;00m(X_train[\u001b[33m'\u001b[39;49;00m\u001b[33mtext\u001b[39;49;00m\u001b[33m'\u001b[39;49;00m]),\n",
      "                                truncation=\u001b[34mTrue\u001b[39;49;00m, \n",
      "                                padding=\u001b[34mTrue\u001b[39;49;00m)\n",
      "    train_dataset = tf.data.Dataset.from_tensor_slices((\u001b[36mdict\u001b[39;49;00m(train_encodings),\n",
      "                                    \u001b[36mlist\u001b[39;49;00m(y_train.values)))\n",
      "\n",
      "    model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
      "\n",
      "    model = TFDistilBertForSequenceClassification.from_pretrained(MODEL_NAME)\n",
      "    \u001b[37m#chose the optimizer\u001b[39;49;00m\n",
      "    \u001b[37m#optimizerr = tf.keras.optimizers.Adam(learning_rate=5e-5)\u001b[39;49;00m\n",
      "    \u001b[37m#define the loss function \u001b[39;49;00m\n",
      "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=\u001b[34m5e-5\u001b[39;49;00m), \n",
      "                  loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=\u001b[34mTrue\u001b[39;49;00m),\n",
      "                  metrics=[\u001b[33m\"\u001b[39;49;00m\u001b[33maccuracy\u001b[39;49;00m\u001b[33m\"\u001b[39;49;00m])\n",
      "    \u001b[37m#losss = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\u001b[39;49;00m\n",
      "    \u001b[37m#build the model\u001b[39;49;00m\n",
      "    \u001b[37m#model.compile(optimizer=optimizerr,\u001b[39;49;00m\n",
      "    \u001b[37m#              loss=losss,\u001b[39;49;00m\n",
      "    \u001b[37m#              metrics=['accuracy'])\u001b[39;49;00m\n",
      "    \u001b[37m# train the model \u001b[39;49;00m\n",
      "    model.fit(train_dataset.shuffle(\u001b[36mlen\u001b[39;49;00m(X_train)).batch(BATCH_SIZE),\n",
      "              epochs=N_EPOCHS,\n",
      "              batch_size=BATCH_SIZE)\n",
      "\n",
      "    model.save_pretrained(args.model_dir)\n",
      "    tokenizer.save_pretrained(args.model_dir)\n",
      "\n",
      "      \n"
     ]
    }
   ],
   "source": [
    "!pygmentize train.py #specify our training script"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now train the model by calling the `fit` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-06-07 03:49:25 Starting - Starting the training job...\n",
      "2022-06-07 03:49:42 Starting - Preparing the instances for trainingProfilerReport-1654573765: InProgress\n",
      "......\n",
      "2022-06-07 03:50:50 Downloading - Downloading input data...\n",
      "2022-06-07 03:51:10 Training - Downloading the training image........................\n",
      "2022-06-07 03:55:12 Training - Training image download completed. Training in progress.\u001b[34m2022-06-07 03:55:15.570141: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:15.577921: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:15.742934: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:15.881884: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:19,283 sagemaker-training-toolkit INFO     Imported framework sagemaker_tensorflow_container.training\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:19,810 sagemaker-training-toolkit INFO     Invoking user script\u001b[0m\n",
      "\u001b[34mTraining Env:\u001b[0m\n",
      "\u001b[34m{\n",
      "    \"additional_framework_parameters\": {},\n",
      "    \"channel_input_dirs\": {\n",
      "        \"train\": \"/opt/ml/input/data/train\"\n",
      "    },\n",
      "    \"current_host\": \"algo-1\",\n",
      "    \"framework_module\": \"sagemaker_tensorflow_container.training:main\",\n",
      "    \"hosts\": [\n",
      "        \"algo-1\"\n",
      "    ],\n",
      "    \"hyperparameters\": {},\n",
      "    \"input_config_dir\": \"/opt/ml/input/config\",\n",
      "    \"input_data_config\": {\n",
      "        \"train\": {\n",
      "            \"TrainingInputMode\": \"File\",\n",
      "            \"S3DistributionType\": \"FullyReplicated\",\n",
      "            \"RecordWrapperType\": \"None\"\n",
      "        }\n",
      "    },\n",
      "    \"input_dir\": \"/opt/ml/input\",\n",
      "    \"is_master\": true,\n",
      "    \"job_name\": \"huggingface-tensorflow-training-2022-06-07-03-49-25-200\",\n",
      "    \"log_level\": 20,\n",
      "    \"master_hostname\": \"algo-1\",\n",
      "    \"model_dir\": \"/opt/ml/model\",\n",
      "    \"module_dir\": \"s3://sagemaker-ap-southeast-2-431579215499/huggingface-tensorflow-training-2022-06-07-03-49-25-200/source/sourcedir.tar.gz\",\n",
      "    \"module_name\": \"train\",\n",
      "    \"network_interface_name\": \"eth0\",\n",
      "    \"num_cpus\": 4,\n",
      "    \"num_gpus\": 1,\n",
      "    \"output_data_dir\": \"/opt/ml/output/data\",\n",
      "    \"output_dir\": \"/opt/ml/output\",\n",
      "    \"output_intermediate_dir\": \"/opt/ml/output/intermediate\",\n",
      "    \"resource_config\": {\n",
      "        \"current_host\": \"algo-1\",\n",
      "        \"current_instance_type\": \"ml.g4dn.xlarge\",\n",
      "        \"current_group_name\": \"homogeneousCluster\",\n",
      "        \"hosts\": [\n",
      "            \"algo-1\"\n",
      "        ],\n",
      "        \"instance_groups\": [\n",
      "            {\n",
      "                \"instance_group_name\": \"homogeneousCluster\",\n",
      "                \"instance_type\": \"ml.g4dn.xlarge\",\n",
      "                \"hosts\": [\n",
      "                    \"algo-1\"\n",
      "                ]\n",
      "            }\n",
      "        ],\n",
      "        \"network_interface_name\": \"eth0\"\n",
      "    },\n",
      "    \"user_entry_point\": \"train.py\"\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mEnvironment variables:\u001b[0m\n",
      "\u001b[34mSM_HOSTS=[\"algo-1\"]\u001b[0m\n",
      "\u001b[34mSM_NETWORK_INTERFACE_NAME=eth0\u001b[0m\n",
      "\u001b[34mSM_HPS={}\u001b[0m\n",
      "\u001b[34mSM_USER_ENTRY_POINT=train.py\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_PARAMS={}\u001b[0m\n",
      "\u001b[34mSM_RESOURCE_CONFIG={\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"}\u001b[0m\n",
      "\u001b[34mSM_INPUT_DATA_CONFIG={\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}}\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DATA_DIR=/opt/ml/output/data\u001b[0m\n",
      "\u001b[34mSM_CHANNELS=[\"train\"]\u001b[0m\n",
      "\u001b[34mSM_CURRENT_HOST=algo-1\u001b[0m\n",
      "\u001b[34mSM_MODULE_NAME=train\u001b[0m\n",
      "\u001b[34mSM_LOG_LEVEL=20\u001b[0m\n",
      "\u001b[34mSM_FRAMEWORK_MODULE=sagemaker_tensorflow_container.training:main\u001b[0m\n",
      "\u001b[34mSM_INPUT_DIR=/opt/ml/input\u001b[0m\n",
      "\u001b[34mSM_INPUT_CONFIG_DIR=/opt/ml/input/config\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_DIR=/opt/ml/output\u001b[0m\n",
      "\u001b[34mSM_NUM_CPUS=4\u001b[0m\n",
      "\u001b[34mSM_NUM_GPUS=1\u001b[0m\n",
      "\u001b[34mSM_MODEL_DIR=/opt/ml/model\u001b[0m\n",
      "\u001b[34mSM_MODULE_DIR=s3://sagemaker-ap-southeast-2-431579215499/huggingface-tensorflow-training-2022-06-07-03-49-25-200/source/sourcedir.tar.gz\u001b[0m\n",
      "\u001b[34mSM_TRAINING_ENV={\"additional_framework_parameters\":{},\"channel_input_dirs\":{\"train\":\"/opt/ml/input/data/train\"},\"current_host\":\"algo-1\",\"framework_module\":\"sagemaker_tensorflow_container.training:main\",\"hosts\":[\"algo-1\"],\"hyperparameters\":{},\"input_config_dir\":\"/opt/ml/input/config\",\"input_data_config\":{\"train\":{\"RecordWrapperType\":\"None\",\"S3DistributionType\":\"FullyReplicated\",\"TrainingInputMode\":\"File\"}},\"input_dir\":\"/opt/ml/input\",\"is_master\":true,\"job_name\":\"huggingface-tensorflow-training-2022-06-07-03-49-25-200\",\"log_level\":20,\"master_hostname\":\"algo-1\",\"model_dir\":\"/opt/ml/model\",\"module_dir\":\"s3://sagemaker-ap-southeast-2-431579215499/huggingface-tensorflow-training-2022-06-07-03-49-25-200/source/sourcedir.tar.gz\",\"module_name\":\"train\",\"network_interface_name\":\"eth0\",\"num_cpus\":4,\"num_gpus\":1,\"output_data_dir\":\"/opt/ml/output/data\",\"output_dir\":\"/opt/ml/output\",\"output_intermediate_dir\":\"/opt/ml/output/intermediate\",\"resource_config\":{\"current_group_name\":\"homogeneousCluster\",\"current_host\":\"algo-1\",\"current_instance_type\":\"ml.g4dn.xlarge\",\"hosts\":[\"algo-1\"],\"instance_groups\":[{\"hosts\":[\"algo-1\"],\"instance_group_name\":\"homogeneousCluster\",\"instance_type\":\"ml.g4dn.xlarge\"}],\"network_interface_name\":\"eth0\"},\"user_entry_point\":\"train.py\"}\u001b[0m\n",
      "\u001b[34mSM_USER_ARGS=[]\u001b[0m\n",
      "\u001b[34mSM_OUTPUT_INTERMEDIATE_DIR=/opt/ml/output/intermediate\u001b[0m\n",
      "\u001b[34mSM_CHANNEL_TRAIN=/opt/ml/input/data/train\u001b[0m\n",
      "\u001b[34mPYTHONPATH=/opt/ml/code:/usr/local/bin:/usr/local/lib/python37.zip:/usr/local/lib/python3.7:/usr/local/lib/python3.7/lib-dynload:/usr/local/lib/python3.7/site-packages\u001b[0m\n",
      "\u001b[34mInvoking script with the following command:\u001b[0m\n",
      "\u001b[34m/usr/local/bin/python3.7 train.py\u001b[0m\n",
      "\u001b[34m[2022-06-07 03:55:58.172 ip-10-0-189-96.ap-southeast-2.compute.internal:27 INFO utils.py:27] RULE_JOB_STOP_SIGNAL_FILENAME: None\u001b[0m\n",
      "\u001b[34m[2022-06-07 03:55:58.252 ip-10-0-189-96.ap-southeast-2.compute.internal:27 INFO profiler_config_parser.py:102] User has disabled profiler.\u001b[0m\n",
      "\u001b[34m[2022-06-07 03:56:02.832 ip-10-0-189-96.ap-southeast-2.compute.internal:27 INFO json_config.py:91] Creating hook from json_config at /opt/ml/input/config/debughookconfig.json.\u001b[0m\n",
      "\u001b[34m[2022-06-07 03:56:02.833 ip-10-0-189-96.ap-southeast-2.compute.internal:27 INFO hook.py:201] tensorboard_dir has not been set for the hook. SMDebug will not be exporting tensorboard summaries.\u001b[0m\n",
      "\u001b[34m[2022-06-07 03:56:02.834 ip-10-0-189-96.ap-southeast-2.compute.internal:27 INFO hook.py:255] Saving to /opt/ml/output/tensors\u001b[0m\n",
      "\u001b[34m[2022-06-07 03:56:02.834 ip-10-0-189-96.ap-southeast-2.compute.internal:27 INFO state_store.py:77] The checkpoint config file /opt/ml/input/config/checkpointconfig.json does not exist.\u001b[0m\n",
      "\u001b[34m[2022-06-07 03:56:02.834 ip-10-0-189-96.ap-southeast-2.compute.internal:27 INFO hook.py:425] Monitoring the collections: sm_metrics, metrics, losses\u001b[0m\n",
      "\u001b[34mEpoch 1/3\u001b[0m\n",
      "\u001b[34m[2022-06-07 03:56:02.874 ip-10-0-189-96.ap-southeast-2.compute.internal:27 INFO hook.py:425] Monitoring the collections: sm_metrics, metrics, losses\u001b[0m\n",
      "\u001b[34m#0151/9 [==>...........................] - ETA: 1:47 - loss: 3.7723 - accuracy: 0.3750#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0152/9 [=====>........................] - ETA: 5s - loss: 2.3784 - accuracy: 0.4688  #010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/9 [=========>....................] - ETA: 4s - loss: 2.0714 - accuracy: 0.4583#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/9 [============>.................] - ETA: 4s - loss: 1.7968 - accuracy: 0.5000#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0155/9 [===============>..............] - ETA: 3s - loss: 1.5705 - accuracy: 0.5250#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0156/9 [===================>..........] - ETA: 2s - loss: 1.4505 - accuracy: 0.5000#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0157/9 [======================>.......] - ETA: 1s - loss: 1.3281 - accuracy: 0.5179#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0158/9 [=========================>....] - ETA: 0s - loss: 1.2221 - accuracy: 0.5625#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0159/9 [==============================] - ETA: 0s - loss: 1.1757 - accuracy: 0.5612#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0159/9 [==============================] - 20s 803ms/step - loss: 1.1757 - accuracy: 0.5612\u001b[0m\n",
      "\u001b[34mEpoch 2/3\u001b[0m\n",
      "\u001b[34m#0151/9 [==>...........................] - ETA: 6s - loss: 0.6031 - accuracy: 0.5000#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0152/9 [=====>........................] - ETA: 5s - loss: 0.6559 - accuracy: 0.4688#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/9 [=========>....................] - ETA: 5s - loss: 0.7241 - accuracy: 0.4167#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/9 [============>.................] - ETA: 4s - loss: 0.6821 - accuracy: 0.4688#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0155/9 [===============>..............] - ETA: 3s - loss: 0.6604 - accuracy: 0.5250#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0156/9 [===================>..........] - ETA: 2s - loss: 0.6343 - accuracy: 0.5938#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0157/9 [======================>.......] - ETA: 1s - loss: 0.6200 - accuracy: 0.6250#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0158/9 [=========================>....] - ETA: 0s - loss: 0.6091 - accuracy: 0.6562#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0159/9 [==============================] - ETA: 0s - loss: 0.5999 - accuracy: 0.6835#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0159/9 [==============================] - 7s 804ms/step - loss: 0.5999 - accuracy: 0.6835\u001b[0m\n",
      "\u001b[34mEpoch 3/3\u001b[0m\n",
      "\u001b[34m#0151/9 [==>...........................] - ETA: 6s - loss: 0.4940 - accuracy: 0.9375#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0152/9 [=====>........................] - ETA: 5s - loss: 0.4570 - accuracy: 0.9375#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0153/9 [=========>....................] - ETA: 5s - loss: 0.4367 - accuracy: 0.9583#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0154/9 [============>.................] - ETA: 4s - loss: 0.4285 - accuracy: 0.9531#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0155/9 [===============>..............] - ETA: 3s - loss: 0.4322 - accuracy: 0.9375#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0156/9 [===================>..........] - ETA: 2s - loss: 0.4183 - accuracy: 0.9167#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0157/9 [======================>.......] - ETA: 1s - loss: 0.4156 - accuracy: 0.9018#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0158/9 [=========================>....] - ETA: 0s - loss: 0.3892 - accuracy: 0.9141#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0159/9 [==============================] - ETA: 0s - loss: 0.3803 - accuracy: 0.9065#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#010#0159/9 [==============================] - 7s 808ms/step - loss: 0.3803 - accuracy: 0.9065\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:20.637062: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:20.637206: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:105] SageMaker Profiler is not enabled. The timeline writer thread will not be started, future recorded events will be dropped.\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:20.678551: W tensorflow/core/profiler/internal/smprofiler_timeline.cc:460] Initializing the SageMaker Profiler.\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]#015Downloading:  18%|█▊        | 41.0k/226k [00:00<00:00, 218kB/s]#015Downloading:  39%|███▉      | 89.0k/226k [00:00<00:00, 237kB/s]#015Downloading:  89%|████████▉ | 201k/226k [00:00<00:00, 396kB/s] #015Downloading: 100%|██████████| 226k/226k [00:00<00:00, 394kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 48.0/48.0 [00:00<00:00, 35.0kB/s]\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/629 [00:00<?, ?B/s]#015Downloading: 100%|██████████| 629/629 [00:00<00:00, 444kB/s]\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\u001b[0m\n",
      "\u001b[34m#015Downloading:   0%|          | 0.00/256M [00:00<?, ?B/s]#015Downloading:   0%|          | 17.0k/256M [00:00<56:26, 79.1kB/s]#015Downloading:   0%|          | 50.0k/256M [00:00<36:34, 122kB/s] #015Downloading:   0%|          | 124k/256M [00:00<15:15, 293kB/s] #015Downloading:   0%|          | 168k/256M [00:00<13:50, 323kB/s]#015Downloading:   0%|          | 305k/256M [00:00<08:25, 529kB/s]#015Downloading:   0%|          | 603k/256M [00:00<03:52, 1.15MB/s]#015Downloading:   0%|          | 736k/256M [00:01<03:57, 1.13MB/s]#015Downloading:   0%|          | 1.24M/256M [00:01<01:58, 2.25MB/s]#015Downloading:   1%|          | 1.49M/256M [00:01<01:56, 2.28MB/s]#015Downloading:   1%|          | 2.56M/256M [00:01<00:56, 4.69MB/s]#015Downloading:   1%|          | 3.05M/256M [00:01<00:57, 4.63MB/s]#015Downloading:   2%|▏         | 4.33M/256M [00:01<00:37, 7.07MB/s]#015Downloading:   2%|▏         | 5.24M/256M [00:01<00:33, 7.79MB/s]#015Downloading:   3%|▎         | 6.68M/256M [00:01<00:26, 9.76MB/s]#015Downloading:   3%|▎         | 7.70M/256M [00:01<00:26, 9.99MB/s]#015Downloading:   4%|▎         | 9.22M/256M [00:01<00:22, 11.5MB/s]#015Downloading:   4%|▍         | 10.3M/256M [00:02<00:23, 10.8MB/s]#015Downloading:   5%|▍         | 11.8M/256M [00:02<00:21, 12.0MB/s]#015Downloading:   5%|▌         | 13.3M/256M [00:02<00:19, 13.0MB/s]#015Downloading:   6%|▌         | 14.5M/256M [00:02<00:19, 13.0MB/s]#015Downloading:   6%|▌         | 15.8M/256M [00:02<00:19, 12.7MB/s]#015Downloading:   7%|▋         | 17.0M/256M [00:02<00:19, 12.8MB/s]#015Downloading:   7%|▋         | 18.2M/256M [00:02<00:19, 12.5MB/s]#015Downloading:   8%|▊         | 19.6M/256M [00:02<00:19, 13.0MB/s]#015Downloading:   8%|▊         | 20.8M/256M [00:02<00:19, 12.5MB/s]#015Downloading:   9%|▊         | 22.3M/256M [00:03<00:18, 13.2MB/s]#015Downloading:   9%|▉         | 23.5M/256M [00:03<00:19, 12.7MB/s]#015Downloading:  10%|▉         | 25.0M/256M [00:03<00:17, 13.5MB/s]#015Downloading:  10%|█         | 26.3M/256M [00:03<00:18, 12.9MB/s]#015Downloading:  11%|█         | 27.8M/256M [00:03<00:17, 13.7MB/s]#015Downloading:  11%|█▏        | 29.2M/256M [00:03<00:18, 13.1MB/s]#015Downloading:  12%|█▏        | 30.7M/256M [00:03<00:16, 13.9MB/s]#015Downloading:  13%|█▎        | 32.0M/256M [00:03<00:17, 13.4MB/s]#015Downloading:  13%|█▎        | 33.6M/256M [00:03<00:16, 14.2MB/s]#015Downloading:  14%|█▎        | 34.9M/256M [00:04<00:17, 13.5MB/s]#015Downloading:  14%|█▍        | 36.5M/256M [00:04<00:15, 14.4MB/s]#015Downloading:  15%|█▍        | 37.9M/256M [00:04<00:16, 13.7MB/s]#015Downloading:  15%|█▌        | 39.5M/256M [00:04<00:15, 14.7MB/s]#015Downloading:  16%|█▌        | 41.0M/256M [00:04<00:16, 14.1MB/s]#015Downloading:  17%|█▋        | 42.6M/256M [00:04<00:14, 14.9MB/s]#015Downloading:  17%|█▋        | 44.0M/256M [00:04<00:15, 14.2MB/s]#015Downloading:  18%|█▊        | 45.7M/256M [00:04<00:14, 15.1MB/s]#015Downloading:  18%|█▊        | 47.2M/256M [00:04<00:15, 14.4MB/s]#015Downloading:  19%|█▉        | 48.7M/256M [00:05<00:16, 12.9MB/s]#015Downloading:  20%|█▉        | 50.6M/256M [00:05<00:14, 14.4MB/s]#015Downloading:  20%|██        | 52.0M/256M [00:05<00:15, 14.2MB/s]#015Downloading:  21%|██        | 53.8M/256M [00:05<00:13, 15.4MB/s]#015Downloading:  22%|██▏       | 55.3M/256M [00:05<00:14, 14.5MB/s]#015Downloading:  22%|██▏       | 57.1M/256M [00:05<00:13, 15.7MB/s]#015Downloading:  23%|██▎       | 58.6M/256M [00:05<00:14, 14.7MB/s]#015Downloading:  24%|██▎       | 60.5M/256M [00:05<00:12, 16.0MB/s]#015Downloading:  24%|██▍       | 62.1M/256M [00:05<00:13, 14.9MB/s]#015Downloading:  25%|██▍       | 63.7M/256M [00:06<00:14, 13.8MB/s]#015Downloading:  26%|██▌       | 65.8M/256M [00:06<00:12, 15.5MB/s]#015Downloading:  26%|██▋       | 67.3M/256M [00:06<00:13, 14.6MB/s]#015Downloading:  27%|██▋       | 69.2M/256M [00:06<00:12, 16.1MB/s]#015Downloading:  28%|██▊       | 70.8M/256M [00:06<00:13, 14.9MB/s]#015Downloading:  28%|██▊       | 72.8M/256M [00:06<00:11, 16.5MB/s]#015Downloading:  29%|██▉       | 74.4M/256M [00:06<00:12, 15.2MB/s]#015Downloading:  30%|██▉       | 76.0M/256M [00:06<00:13, 13.7MB/s]#015Downloading:  31%|███       | 78.2M/256M [00:07<00:11, 15.7MB/s]#015Downloading:  31%|███       | 79.8M/256M [00:07<00:12, 14.9MB/s]#015Downloading:  32%|███▏      | 81.8M/256M [00:07<00:10, 16.6MB/s]#015Downloading:  33%|███▎      | 83.5M/256M [00:07<00:11, 15.7MB/s]#015Downloading:  33%|███▎      | 85.5M/256M [00:07<00:10, 17.3MB/s]#015Downloading:  34%|███▍      | 87.2M/256M [00:07<00:10, 16.1MB/s]#015Downloading:  35%|███▍      | 89.1M/256M [00:07<00:11, 14.8MB/s]#015Downloading:  36%|███▌      | 91.4M/256M [00:07<00:10, 17.0MB/s]#015Downloading:  36%|███▋      | 93.1M/256M [00:08<00:10, 15.7MB/s]#015Downloading:  37%|███▋      | 95.3M/256M [00:08<00:09, 17.5MB/s]#015Downloading:  38%|███▊      | 97.1M/256M [00:08<00:09, 16.7MB/s]#015Downloading:  39%|███▉      | 99.3M/256M [00:08<00:10, 15.6MB/s]#015Downloading:  40%|███▉      | 102M/256M [00:08<00:09, 17.8MB/s] #015Downloading:  40%|████      | 103M/256M [00:08<00:09, 16.5MB/s]#015Downloading:  41%|████▏     | 106M/256M [00:08<00:08, 18.5MB/s]#015Downloading:  42%|████▏     | 108M/256M [00:08<00:09, 17.1MB/s]#015Downloading:  43%|████▎     | 110M/256M [00:09<00:09, 16.5MB/s]#015Downloading:  44%|████▍     | 112M/256M [00:09<00:08, 18.6MB/s]#015Downloading:  45%|████▍     | 114M/256M [00:09<00:08, 16.8MB/s]#015Downloading:  46%|████▌     | 117M/256M [00:09<00:07, 18.7MB/s]#015Downloading:  46%|████▋     | 119M/256M [00:09<00:08, 17.5MB/s]#015Downloading:  47%|████▋     | 121M/256M [00:09<00:07, 19.4MB/s]#015Downloading:  48%|████▊     | 123M/256M [00:09<00:07, 18.1MB/s]#015Downloading:  49%|████▉     | 125M/256M [00:09<00:07, 17.4MB/s]#015Downloading:  50%|████▉     | 127M/256M [00:10<00:07, 18.7MB/s]#015Downloading:  50%|█████     | 129M/256M [00:10<00:08, 16.4MB/s]#015Downloading:  51%|█████▏    | 131M/256M [00:10<00:06, 18.8MB/s]#015Downloading:  52%|█████▏    | 133M/256M [00:10<00:07, 16.6MB/s]#015Downloading:  53%|█████▎    | 135M/256M [00:10<00:08, 14.5MB/s]#015Downloading:  54%|█████▎    | 137M/256M [00:10<00:07, 17.2MB/s]#015Downloading:  54%|█████▍    | 139M/256M [00:10<00:07, 16.6MB/s]#015Downloading:  55%|█████▌    | 142M/256M [00:10<00:06, 19.1MB/s]#015Downloading:  56%|█████▌    | 144M/256M [00:11<00:06, 17.9MB/s]#015Downloading:  57%|█████▋    | 146M/256M [00:11<00:06, 16.7MB/s]#015Downloading:  58%|█████▊    | 149M/256M [00:11<00:05, 19.4MB/s]#015Downloading:  59%|█████▉    | 151M/256M [00:11<00:06, 17.8MB/s]#015Downloading:  60%|█████▉    | 153M/256M [00:11<00:06, 15.8MB/s]#015Downloading:  61%|██████    | 156M/256M [00:11<00:05, 18.7MB/s]#015Downloading:  62%|██████▏   | 157M/256M [00:11<00:05, 17.4MB/s]#015Downloading:  63%|██████▎   | 160M/256M [00:11<00:05, 19.8MB/s]#015Downloading:  63%|██████▎   | 162M/256M [00:12<00:05, 18.1MB/s]#015Downloading:  64%|██████▍   | 164M/256M [00:12<00:05, 16.5MB/s]#015Downloading:  65%|██████▌   | 167M/256M [00:12<00:04, 19.4MB/s]#015Downloading:  66%|██████▌   | 169M/256M [00:12<00:05, 18.0MB/s]#015Downloading:  67%|██████▋   | 172M/256M [00:12<00:05, 17.1MB/s]#015Downloading:  68%|██████▊   | 174M/256M [00:12<00:04, 19.9MB/s]#015Downloading:  69%|██████▉   | 176M/256M [00:12<00:04, 17.6MB/s]#015Downloading:  70%|██████▉   | 178M/256M [00:13<00:05, 15.5MB/s]#015Downloading:  71%|███████   | 181M/256M [00:13<00:04, 18.6MB/s]#015Downloading:  72%|███████▏  | 183M/256M [00:13<00:04, 17.3MB/s]#015Downloading:  73%|███████▎  | 186M/256M [00:13<00:03, 19.8MB/s]#015Downloading:  74%|███████▎  | 188M/256M [00:13<00:04, 17.5MB/s]#015Downloading:  74%|███████▍  | 190M/256M [00:13<00:04, 14.6MB/s]#015Downloading:  75%|███████▌  | 192M/256M [00:14<00:04, 15.3MB/s]#015Downloading:  76%|███████▋  | 195M/256M [00:14<00:03, 18.4MB/s]#015Downloading:  77%|███████▋  | 197M/256M [00:14<00:03, 16.7MB/s]#015Downloading:  78%|███████▊  | 199M/256M [00:14<00:04, 14.5MB/s]#015Downloading:  79%|███████▉  | 202M/256M [00:14<00:03, 17.5MB/s]#015Downloading:  80%|███████▉  | 204M/256M [00:14<00:03, 16.5MB/s]#015Downloading:  81%|████████  | 206M/256M [00:14<00:03, 15.6MB/s]#015Downloading:  82%|████████▏ | 209M/256M [00:14<00:02, 18.7MB/s]#015Downloading:  82%|████████▏ | 211M/256M [00:15<00:02, 17.5MB/s]#015Downloading:  83%|████████▎ | 213M/256M [00:15<00:02, 15.4MB/s]#015Downloading:  84%|████████▍ | 216M/256M [00:15<00:02, 18.6MB/s]#015Downloading:  85%|████████▌ | 218M/256M [00:15<00:02, 17.3MB/s]#015Downloading:  86%|████████▌ | 220M/256M [00:15<00:01, 20.0MB/s]#015Downloading:  87%|████████▋ | 222M/256M [00:15<00:01, 18.4MB/s]#015Downloading:  88%|████████▊ | 224M/256M [00:15<00:02, 16.4MB/s]#015Downloading:  89%|████████▉ | 227M/256M [00:16<00:01, 19.2MB/s]#015Downloading:  90%|████████▉ | 229M/256M [00:16<00:01, 18.0MB/s]#015Downloading:  91%|█████████ | 232M/256M [00:16<00:01, 20.3MB/s]#015Downloading:  91%|█████████▏| 234M/256M [00:16<00:01, 17.9MB/s]#015Downloading:  92%|█████████▏| 236M/256M [00:16<00:01, 16.6MB/s]#015Downloading:  93%|█████████▎| 239M/256M [00:16<00:00, 19.5MB/s]#015Downloading:  94%|█████████▍| 241M/256M [00:16<00:00, 18.2MB/s]#015Downloading:  95%|█████████▌| 243M/256M [00:16<00:00, 20.7MB/s]#015Downloading:  96%|█████████▌| 245M/256M [00:17<00:00, 19.0MB/s]#015Downloading:  97%|█████████▋| 247M/256M [00:17<00:00, 16.6MB/s]#015Downloading:  98%|█████████▊| 250M/256M [00:17<00:00, 19.3MB/s]#015Downloading:  99%|█████████▊| 252M/256M [00:17<00:00, 17.9MB/s]#015Downloading: 100%|█████████▉| 254M/256M [00:17<00:00, 16.5MB/s]#015Downloading: 100%|██████████| 256M/256M [00:17<00:00, 15.1MB/s]\u001b[0m\n",
      "\u001b[34m2022-06-07 03:55:57.480936: W tensorflow/python/util/util.cc:348] Sets are not currently considered sequences, but this may change in the future, so consider avoiding using them.\u001b[0m\n",
      "\u001b[34mAll model checkpoint layers were used when initializing TFDistilBertForSequenceClassification.\u001b[0m\n",
      "\u001b[34mAll the layers of TFDistilBertForSequenceClassification were initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english.\u001b[0m\n",
      "\u001b[34mIf your task is similar to the task the model of the checkpoint was trained on, you can already use TFDistilBertForSequenceClassification for predictions without further training.\u001b[0m\n",
      "\u001b[34m/usr/local/lib/python3.7/site-packages/transformers/configuration_utils.py:337: UserWarning: Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 Transformers. Using `model.gradient_checkpointing_enable()` instead, or if you are using the `Trainer` API, pass `gradient_checkpointing=True` in your `TrainingArguments`.\n",
      "  \"Passing `gradient_checkpointing` to a config initialization is deprecated and will be removed in v5 \"\u001b[0m\n",
      "\u001b[34mSome layers from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english were not used when initializing TFDistilBertForSequenceClassification: ['dropout_19']\u001b[0m\n",
      "\u001b[34m- This IS expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\u001b[0m\n",
      "\u001b[34m- This IS NOT expected if you are initializing TFDistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\u001b[0m\n",
      "\u001b[34mSome layers of TFDistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased-finetuned-sst-2-english and are newly initialized: ['dropout_39']\u001b[0m\n",
      "\u001b[34mYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThe `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/array_ops.py:5049: calling gather (from tensorflow.python.ops.array_ops) with validate_indices is deprecated and will be removed in a future version.\u001b[0m\n",
      "\u001b[34mInstructions for updating:\u001b[0m\n",
      "\u001b[34mThe `validate_indices` argument has no effect. Indices are always validated on CPU and never validated on GPU.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3332s vs `on_train_batch_end` time: 0.4156s). Check your callbacks.\u001b[0m\n",
      "\u001b[34mWARNING:tensorflow:Callback method `on_train_batch_end` is slow compared to the batch time (batch time: 0.3332s vs `on_train_batch_end` time: 0.4156s). Check your callbacks.\u001b[0m\n",
      "\u001b[34m2022-06-07 03:56:38,977 sagemaker_tensorflow_container.training WARNING  Your model will NOT be servable with SageMaker TensorFlow Serving container. The model artifact was not saved in the TensorFlow SavedModel directory structure:\u001b[0m\n",
      "\u001b[34mhttps://www.tensorflow.org/guide/saved_model#structure_of_a_savedmodel_directory\u001b[0m\n",
      "\u001b[34m2022-06-07 03:56:38,978 sagemaker-training-toolkit INFO     Reporting training SUCCESS\u001b[0m\n",
      "\n",
      "2022-06-07 03:56:49 Uploading - Uploading generated training model\n",
      "2022-06-07 03:57:49 Completed - Training job completed\n",
      "ProfilerReport-1654573765: NoIssuesFound\n",
      "Training seconds: 403\n",
      "Billable seconds: 403\n"
     ]
    }
   ],
   "source": [
    "huggingface_estimator.fit(\n",
    "  {'train': f's3://{BUCKET}/{PREFIX}/train.csv'}\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy the Model as an endpoint\n",
    "Now we will deploy the model as an endpoint, which can be queried with independent data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----!"
     ]
    }
   ],
   "source": [
    "endpoint=huggingface_estimator.deploy(1,\"ml.g4dn.xlarge\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Invoke the endpoint with test data\n",
    "We will pass some holdout data to the endpoint to get an estimate of performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'[{\"label\":\"NEGATIVE\",\"score\":0.8951427936553955}]'\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from sagemaker.serializers import JSONSerializer\n",
    "my_serializer=JSONSerializer()\n",
    "my_predictor=sagemaker.predictor.Predictor(endpoint.endpoint_name,sagemaker_session=sagemaker_session,serializer=my_serializer)\n",
    "the_inputs=X_test['text'].tolist()\n",
    "\n",
    "all_results=[]\n",
    "for i in range(0,len(the_inputs)):\n",
    "    the_input= the_inputs[i][0:512] #truncate to 512 characters\n",
    "    the_result=my_predictor.predict({\"inputs\":the_input})\n",
    "    all_results.append(the_result)\n",
    "print(all_results[0]) # see what one result looks like.\n",
    "#if the predicted label is negative normalize subtract it from 1, \n",
    "#so that lower scores mean predictions of General Medicine, and higher scores mean prediction of Radioligy.\n",
    "all_results_2=[]\n",
    "for i in all_results:\n",
    "    the_result= json.loads(i)[0]\n",
    "    the_score=the_result['score']\n",
    "    the_label=the_result['label']\n",
    "    if the_label==\"NEGATIVE\":\n",
    "        all_results_2.append(1-the_score)\n",
    "    else:\n",
    "        all_results_2.append(the_score)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Measure the performance using a ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn.metrics as metrics\n",
    "# calculate the fpr and tpr for all thresholds of the classification\n",
    "preds=all_results_2\n",
    "fpr, tpr, threshold = metrics.roc_curve(X_test['specialty_encoded'].tolist(), preds)\n",
    "roc_auc = metrics.auc(fpr, tpr)\n",
    "\n",
    "# method I: plt\n",
    "import matplotlib.pyplot as plt\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n",
    "plt.legend(loc = 'lower right')\n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that due to the fact that this is a small dataset, you may get a performance of .95 or even higher."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.g4dn.4xlarge",
  "kernelspec": {
   "display_name": "conda_tensorflow2_p38",
   "language": "python",
   "name": "conda_tensorflow2_p38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
